\chapter{Classification Methods and Vector Space}

\section{Logistic Regression and Sentiment Analysis}

\subsection{Bag of Word and Sparse Representation} 

\paragraph{Features:} 
Bag of Word approach to generate features that are 1 or 0 to indicate whether each word are in the specific training example. 

\paragraph{Algorithm:} 
Logistic regression on the features to classify as positive or negative sentiment. 

\paragraph{Problem:} 
Sparse features. Forces logistic regression to have large number of parameters. 


\subsection{Negative and Positive Frequency Representation} 
\paragraph{Features:} 
For each word, identify its positive and negative frequency by counting how many times the word appear in positive cases and negative cases. The final feature for an example is the a vector of length three: a bias unit (normally 1), the sum of dedupped words' positive frequency, and the sum of dedupped words' negative frequency 

\paragraph{Algorithm:} 
Logistic regression on the features to classify as positive or negative sentiment. 





\section{Naive Bayes and Sentiment Analysis} 
\paragraph{Features:} For each word, derive its positive and negative frequency, e.g.: 
    \begin{align*}
        & p(w_i|pos) = \frac{freq(w_i, pos)}{freq(pos)}\\
        & freq(w_i, pos) \text{: Total count of word i in all positive text}\\
        & freq(pos) \text{: Total count of all words in all positive text}
    \end{align*}

\paragraph{Algorithms:} Naive Bayes 
    \begin{equation*}
        \indic{\frac{p(pos)}{p(neg)}\prod_{i=1}^n \frac{p(w_i|pos)}{p(w_i|neg)} \geq 1} 
    \end{equation*}


\paragraph{Features: Laplacian Smoothing} The aforementioned feature suffers the problem of multiplying / dividing by 0 if certain word only shows up in one class. Laplacian smoothing replaces the the conditional probability for word i with 
    \begin{align*}
        & p_{L}(w_i|pos) = \frac{freq(w_i, pos) + 1}{N_{pos} + V}\\
        & N_{pos} \text{: Sum of word frequency in positive class} \\
        & V \text{: Number of unique word in the entire text (both classes)} 
    \end{align*}

\paragraph{Tricks: Log sum}: Direct likelihood generally is a very small number and multiplying them risk of underflow. So we use log of the number to transform multiplication into addition for safer calculation. 
    \begin{align*}
        \log\left( \frac{p(pos)}{p(neg)}\prod_{i=1}^n \frac{p(w_i|pos)}{p(w_i|neg)} \right) = \log \frac{p(pos)}{p(neg)} + \left( \sum_{i=1}^n \log \frac{p(w_i|pos)}{p(w_i|neg)} \right)
    \end{align*}
The threshold becomes $0$ instead of $1$ after transformation. 




\section{Vector Space Model} 
\subsection{Word to Word and Word to Doc Design} 
\paragraph{Word to Word Design}: A matrix with words on the rows and columns. Cell value are the number of times two words (row \& col) co-occurance within $k$ distance away from each other. 

\paragraph{Word to Doc Design}: A matrix with words on the rows and document types as columns. The cell value are the number of times certain words appear in certain genre of documents. 


\subsection{PCA}
PCA as identifying rotation matrix: \href{https://drive.google.com/file/d/1M7jtWcFcK-eakfO47cCssHSuuiZyS9TK/view?usp=sharing}{Link}
\begin{enumerate}
    \item Deriving uncorrelated features through eigenvector
        \begin{enumerate}
            \item Mean Normalize data
            \item Get covariance matrix 
            \item Perform SVD to get eigenvector matrix (first matrix) and eigenvalue (diagonal value of second matrix)
            \item Eigenvector should be organized according to descending eigenvalue
        \end{enumerate}
    \item Dot product to project data matrix $X$ to the first $k$ column of eigenvector matrix $U$ through $X' = XU[:k]$
    \item Calculate percentage of variance explained via $\frac{\sum_{i=1}^{d} S_{ii}}{\sum_{j=1}^k S_{jj}}$
\end{enumerate}


\section{Machine Translation with K Nearest Neighbor and Locality Sensitive Hashing}

\subsection{Machine Translation Key Idea} 
Let $X$ be a matrix $m \times n$ with each row as an English word, let $Y$ be a matrix with each row as the corresponding French word. The translation matrix $R$ is the matrix that minimizes the squared Frobenius norm of the difference: 
    \begin{align*}
        \norm{XR - Y}_F^2
    \end{align*}
$R$ can be obtained by iterative update via gradient:  
    \begin{align*}
        & R = R - \alpha * g \\
        & g = \frac{2}{m}(X^T(XR-Y))
    \end{align*}


\subsection{Locality Sensitive Hashing Example}
We can define $n$ hyperplanes with normal vector matrix $V$. Each word can be represented with $w_i = 0$ or $w_i = 1$ as below or above the hyperplane $i$. The final hash value of each word can be calculated as $\sum_{i=0}^n 2^i * w_i$. \\

So overall, we can determine the number of hash bucket $h$ (i.e.: the hash value) first, then generate number of hyperplanes $n$ based on the determined number of bucket ($2^n = h$). For each set of hyperplanes, if two items have the same hash value, they are in the same bucket. We can repeat this process for many times to improve accuracy. 

\subsection{Approximate Nearest Neighbor with LSH} 




\section{Code Example}
\subsection{Basic nltk and preprocessing} 

\paragraph{Loading Data from NLTK} \mbox{}
\begin{python}
import nltk                             
from nltk.corpus import twitter_samples  
import matplotlib.pyplot as plt
import random      

# downloads sample twitter dataset.
nltk.download('twitter_samples')

# use reader to read in positive and negative tweets
# object is a list of strings
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')
\end{python}

\paragraph{Tokenizer} \mbox{}

\begin{python}
from nltk.tokenize import TweetTokenizer 
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
tweet_tokens = tokenizer.tokenize(tweet2) # a string
\end{python}

\paragraph{Steming} \mbox{}
\begin{python}
from nltk.stem import PorterStemmer    
stemmer = PorterStemmer()
stem_word = stemmer.stem(word)  # a word
\end{python}

\subsection{Visualization}
\paragraph{Confidence Ellipse} \href{https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html}{Link} \mbox{}
\begin{python}
confidence_ellipse(data_pos.positive, data_pos.negative, \
    ax, n_std=2, edgecolor='black', label=r'$2\sigma$' )
# data_pos.positive is a sequence of x-axis coordinate
\end{python}





\section{Completed Notebook}

\paragraph{Logistic Regression Sentiment Analysis Notebook}: 
\href{https://drive.google.com/file/d/1_7XX9QWWPlxo43Fw1xxx9PzFxpl27q2D/view?usp=sharing}{Link}
    \begin{itemize}
        \item Stemming, Removing Stopwords
        \item Manual Implementation of Logistic Regression and cost derivation 
    \end{itemize}
    
\paragraph{Naive Bayes Sentiment Analysis Notebook}: 
\href{https://drive.google.com/file/d/1G9ibr3t05UOVmuzRFZZQfG1HO_JkNy-M/view?usp=sharing}{Link}
    \begin{itemize}
        \item Naive Bayes Sentiment Analysis
    \end{itemize}


\paragraph{Vector Space Model Notebook}:
\href{https://drive.google.com/file/d/1BIm71QmfUo2MU9pw2CmmKQfKt1vwn-w6/view?usp=sharing}{Link}
    \begin{itemize}
        \item Manual PCA 
    \end{itemize}

\paragraph{Machine Translation and Nearest Neighbor Search Notebook}:
\href{https://drive.google.com/file/d/1ZQX4BSOBvFa3MQnUNnhGGRkQg2UoDc3S/view?usp=sharing}{Link}
    \begin{itemize}
        \item LSH and approximate neighborhood 
        \item Manual Gradient Descent 
    \end{itemize}